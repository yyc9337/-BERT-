# BERT fine-tuning method for downstream tasks

파인 튜닝은 BERT를 처음부터 학습시키지 않는다는 것을 의마한다. 그 대신 사전 학습된 BERT를 기반으로 태스크에 맞게 가중치를 업데이트 하게된다.
이로써 Kaggle, Dacon과 같은 NLP_competition에서 가장 중요한 작업이라고 할 수 있을 것이다. (최근 몇년 NLP계열 S.O.T.A는 BERT가 싹쓸이하기 때문)

총 4가지의 다운스트림 태스크를 다룰 예정이다.

1. 텍스트 분류
2. 자연어 추론
3. 개체명 인식
4. 질문 - 응답


## 1. 텍스트 분류
사전 학습된 BERT 모델을 텍스트 분류 태스크에 맞춰 파인 튜닝하는 방법이다. 감정 분석을 수행하고 있다고 가정해보겠다. 
감정 분석 태스크에서 목표는 문장이 긍정인지 부정인지 분류하는 것이다. 레이블과 함께 문장이 포함된 데이터셋이 있다고 가정한다.

'I love paris'라는 문장이 주어졌을때 먼저 문장을 토큰화하고 시작 부분에[CLS]토큰의 임베딩만 취한다. [CLS]토큰을 추가한 뒤
문장 끝에 [SEP]토큰을 추가한다. 그 다음 사전 학습된 BERT 모델에 대한 입력으로 토큰을 입력하고 모든 토큰의 임베딩을 가져온다.

다른 모든 토큰의 임베딩을 무시하고 R[CLS](CLS토큰의 임베딩 값)인 [CLS]토큰의 임베딩만 취한다. [CLS]토큰을 포함하면 문장의
집계 표현이 유지된다.R[CLS]를 분류기(소프트맥스 함수가 있는 피드포워드 네트워크)에 입력하고 학습시켜 감정 분석을 수행한다.

![스크린샷 2022-05-20 오후 1 03 16](https://user-images.githubusercontent.com/69188513/169448072-4513f4c1-6efb-4701-8f9a-c073b88d783c.png) <br>
이러한 그림으로 표현될 수 있을 것 같다.

## 2. 자연어 추론
자연어 추론에서 모델은 가정이 주진 전제에 참인지 거짓인지 중립인지 여부를 결정하는 태스크다. 
![img.png](img.png)<br>
먼저 문자 쌍을 토큰화한 다음 첫번째 문장의 시작 부분에 [CLS]토큰을 추가하고 모든 문장 끝에 [SEP]토큰을 추가한다.
후에 사전 학습된 BERT에 토큰을 입력하고 각 토큰의 임베딩을 가져온다. [CLS] 토큰의 표현이 집계 표현을 보유한다.

따라서 [CLS]토큰의 R[CLS](CLS토큰의 임베딩 값) 표현 벡터를 가져와 분류기(피드포워드 + 소프트맥스)에 입력하면, 분류기는 문장이 참, 거짓,
중립일 확률을 반환한다. 학습 초기에는 결과가 정확하지 않지만 여러 번 반복하면 정확한 결과를 얻을 수 있다.

## 3. 질문-응답
질문 - 응답 태스크에서는 질문에 대한 응답이 포함된 단락과 함께 질문이 제공된다. 태스크의 목표는 주어진 질문에 대한 단락에서 답을 추출하는 것이다.

BERT의 입력은 질문-단락 쌍이다. 즉, BERT에 질문과 응답을 담은 단락을 입력하고 단락에서 응답을 추출해야 한다. 따라서 BERT는 단락에서 응답에
해당하는 텍스트의 범위를 반환해야 한다.

## 4. 개체명 인식
개체명 인식에서 우리의 목표는 개체명을 미리 정의된 범주로 분류하는 것이다. 예를들어 '영찬이는 서울에 산다'라는 문장을 예로 들어보면 이 문장에서 
'영찬'은 사람으로, '서울'은 위치로 분류한다.

먼저 문장을 토큰화한 다음 시작 부분에[CLS]토큰을 추가하고 끝에 [SEP]토큰을 추가한다. 그런 다음 사전 학습된 BERT 모델에 토큰을 입력하고 모든 
토큰의 표현 벡터를 얻는다. 그리고 이러한 토큰 표현을 분류기(피드포워드 네트워크 + 소프트맥스 함수)에 입력한다. 그러면 분류기는 개체명이 속한 범주를
반환한다.

![스크린샷 2022-05-20 오후 5 37 55](https://user-images.githubusercontent.com/69188513/169489311-ea22bac8-2b87-44be-8080-3567e8f0dc52.png)




