# BERT fine-tuning method for downstream tasks

파인 튜닝은 BERT를 처음부터 학습시키지 않는다는 것을 의마한다. 그 대신 사전 학습된 BERT를 기반으로 태스크에 맞게 가중치를 업데이트 하게된다.
이로써 Kaggle, Dacon과 같은 NLP_competition에서 가장 중요한 작업이라고 할 수 있을 것이다. (최근 몇년 NLP계열 S.O.T.A는 BERT가 싹쓸이하기 때문)

총 4가지의 다운스트림 태스크를 다룰 예정이다.

1. 텍스트 분류
2. 자연어 추론
3. 개체명 인식
4. 질문 - 응답


## 1. 텍스트 분류
사전 학습된 BERT 모델을 텍스트 분류 태스크에 맞춰 파인 튜닝하는 방법이다. 감정 분석을 수행하고 있다고 가정해보겠다. 
감정 분석 태스크에서 목표는 문장이 긍정인지 부정인지 분류하는 것이다. 레이블과 함께 문장이 포함된 데이터셋이 있다고 가정한다.

'I love paris'라는 문장이 주어졌을때 먼저 문장을 토큰화하고 시작 부분에[CLS]토큰의 임베딩만 취한다. [CLS]토큰을 추가한 뒤
문장 끝에 [SEP]토큰을 추가한다. 그 다음 사전 학습된 BERT 모델에 대한 입력으로 토큰을 입력하고 모든 토큰의 임베딩을 가져온다.

다른 모든 토큰의 임베딩을 무시하고 R[CLS](CLS토큰의 임베딩 값)인 [CLS]토큰의 임베딩만 취한다. [CLS]토큰을 포함하면 문장의
집계 표현이 유지된다.R[CLS]를 분류기(소프트맥스 함수가 있는 피드포워드 네트워크)에 입력하고 학습시켜 감정 분석을 수행한다.

![스크린샷 2022-05-20 오후 1 03 16](https://user-images.githubusercontent.com/69188513/169448072-4513f4c1-6efb-4701-8f9a-c073b88d783c.png) <br>
이러한 그림으로 표현될 수 있을 것 같다.

## 2. 자연어 추론
자연어 추론에서 모델은 가정이 주진 전제에 참인지 거짓인지 중립인지 여부를 결정하는 태스크다.